{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756d50eb",
   "metadata": {},
   "source": [
    "# Lab 1 Big Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43e359",
   "metadata": {},
   "source": [
    "## Processing DBLP dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf66240",
   "metadata": {},
   "source": [
    "## installing necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c289196",
   "metadata": {},
   "source": [
    "Note: it is necessary to have the XMLToCSV function in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e723eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429f9a2",
   "metadata": {},
   "source": [
    "## importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa050cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from lxml import etree\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import lxml.etree as ET\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f071c77",
   "metadata": {},
   "source": [
    "## Setting working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2548a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current working directory: Lab_1\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Define paths relative to cwd\n",
    "script = os.path.join(cwd, \"dblp-to-csv\", \"XMLToCSV.py\")\n",
    "xml_file = os.path.join(cwd, \"dblp.xml\")\n",
    "dtd_file = os.path.join(cwd, \"dblp.dtd\")\n",
    "output_file = os.path.join(cwd, \"dblp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b98f34",
   "metadata": {},
   "source": [
    "## Using python script to parse the XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e51e0a",
   "metadata": {},
   "source": [
    "note: this takes approx. 10 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "312dfb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "Reading elements from DTD file...\n",
      "Finding unique attributes for all elements...\n",
      "Opening output files...\n",
      "Parsing XML and writing to CSV files...\n",
      "Done after 526.742296 seconds\n"
     ]
    }
   ],
   "source": [
    "!python \"{script}\" \"{xml_file}\" \"{dtd_file}\" \"{output_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc7754",
   "metadata": {},
   "source": [
    "## Deleting non-necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9cbedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: dblp_book.csv\n",
      "Deleted: dblp_data.csv\n",
      "Deleted: dblp_incollection.csv\n",
      "Deleted: dblp_mastersthesis.csv\n",
      "Deleted: dblp_phdthesis.csv\n",
      "Deleted: dblp_proceedings.csv\n",
      "Deleted: dblp_www.csv\n",
      "Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "# List of files you want to keep\n",
    "files_to_keep = {\"dblp_article.csv\", \"dblp_inproceedings.csv\"}\n",
    "\n",
    "# List all files in the current directory\n",
    "for filename in os.listdir():\n",
    "    # Delete only CSV files not in the keep list\n",
    "    if filename.endswith(\".csv\") and filename not in files_to_keep:\n",
    "        os.remove(filename)\n",
    "        print(f\"Deleted: {filename}\")\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f229e",
   "metadata": {},
   "source": [
    "## Cleaning and sampling the CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3eda9",
   "metadata": {},
   "source": [
    "we are keeping only the first 4k papers because of computational power limitations. The cleaning part handles bad quotations cases within the csv, e.g.: (' \" ', instead of '') and also deletes empty rows (some rows are empty in the input csv's because of XMLToCSV.py processing).\n",
    "\n",
    "Adittionally, it deletes titles that are equal to \"Preface.\" and \"Editorial.\" which are not real papers but short introductions of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57249450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned 2000 rows from dblp_article.csv into a DataFrame.\n",
      "‚úÖ Cleaned 2000 rows from dblp_inproceedings.csv into a DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Function to clean and trim CSV files\n",
    "def clean_and_trim_csv_to_df(input_file, max_rows=2000):\n",
    "    cleaned_rows = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile, delimiter=';')\n",
    "        header = next(reader)\n",
    "        clean_header = [field.replace('\"', '').strip() for field in header]\n",
    "\n",
    "        # Find indexes for required columns\n",
    "        try:\n",
    "            title_idx = clean_header.index('title')\n",
    "            author_idx = clean_header.index('author')\n",
    "            author_orcid_idx = clean_header.index('author-orcid')\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Missing required column: {e}\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not any(field.strip() for field in row):\n",
    "                continue\n",
    "\n",
    "            clean_row = [field.replace('\"', '').strip() for field in row]\n",
    "\n",
    "            if (title_idx >= len(clean_row) or clean_row[title_idx] == '' or\n",
    "                author_idx >= len(clean_row) or clean_row[author_idx] == '' or\n",
    "                author_orcid_idx >= len(clean_row) or clean_row[author_orcid_idx] == ''):\n",
    "                continue\n",
    "\n",
    "            # Clean title\n",
    "            clean_row[title_idx] = clean_row[title_idx].replace('Preface.', '').strip()\n",
    "\n",
    "            # Filter out unwanted titles\n",
    "            title = clean_row[title_idx]\n",
    "            if title == \"Editorial.\":\n",
    "                continue\n",
    "            if title.startswith(\"Editorial:\"):\n",
    "                continue\n",
    "            if title.strip() == \"\":\n",
    "                continue  # Extra safeguard for empty titles\n",
    "\n",
    "            cleaned_rows.append(clean_row)\n",
    "            if len(cleaned_rows) >= max_rows:\n",
    "                break\n",
    "\n",
    "    # Remove completely empty columns\n",
    "    transposed = list(zip(*cleaned_rows))\n",
    "    non_empty_column_indexes = [i for i, col in enumerate(transposed) if any(field.strip() for field in col)]\n",
    "\n",
    "    final_header = [clean_header[i] for i in non_empty_column_indexes]\n",
    "    final_rows = [[row[i] for i in non_empty_column_indexes] for row in cleaned_rows]\n",
    "\n",
    "    df = pd.DataFrame(final_rows, columns=final_header)\n",
    "    print(f\"‚úÖ Cleaned {len(final_rows)} rows from {input_file} into a DataFrame.\")\n",
    "    return df\n",
    "\n",
    "# Clean and load into DataFrames\n",
    "articles_df = clean_and_trim_csv_to_df('dblp_article.csv')\n",
    "inproceedings_df = clean_and_trim_csv_to_df('dblp_inproceedings.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6f839",
   "metadata": {},
   "source": [
    "# Generating fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689d2af",
   "metadata": {},
   "source": [
    "## Fake data for inproceedings (type, venue) consistent across papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89128f",
   "metadata": {},
   "source": [
    "This code generates consistent fake venue and type data for inproceedings. With consistent we are refering to: inproceedings \"events\" are uniquely identfied by the concatenation of booktitle_year, so if a 2 papers were published within the same event, they will share exactly the same venue and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fbf459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Step 1: Define a list of fake city names\n",
    "venues = [\n",
    "    'Barcelona', 'New York', 'Berlin', 'Tokyo', 'Paris',\n",
    "    'London', 'San Francisco', 'Lisbon', 'Amsterdam', 'Singapore'\n",
    "]\n",
    "\n",
    "# Step 2: Generate a mapping for each (booktitle, year) pair\n",
    "group_keys = inproceedings_df[['booktitle', 'year']].drop_duplicates().copy()\n",
    "group_keys['type'] = group_keys.apply(lambda _: random.choice(['conference', 'workshop']), axis=1)\n",
    "group_keys['venue'] = group_keys.apply(lambda _: random.choice(venues), axis=1)\n",
    "\n",
    "# Step 3: Merge this mapping back into the main DataFrame\n",
    "inproceedings_df_fake = inproceedings_df.merge(group_keys, on=['booktitle', 'year'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8702b",
   "metadata": {},
   "source": [
    "# Creating Abstract, Keywords and Citations between papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616358c",
   "metadata": {},
   "source": [
    "We are generating here a random abstract and a random assignment of keywords of papers.Later, within cypher we will link these papers together using the graph theory, as it is much more faster than checking the relations one by one and creating a citation's csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80900171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Abstracts with faker\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "Faker.seed(42)  # Optional: for reproducibility\n",
    "\n",
    "# Function to generate fake abstract with multiple sentences\n",
    "def generate_fake_abstract(num_sentences=5):\n",
    "    return ' '.join(fake.paragraphs(nb=random.randint(3, num_sentences)))\n",
    "\n",
    "# Apply the abstract generation\n",
    "inproceedings_df_fake['abstract'] = inproceedings_df_fake.apply(lambda _: generate_fake_abstract(), axis=1)\n",
    "articles_df['abstract'] = articles_df.apply(lambda _: generate_fake_abstract(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f827482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Keywords\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Keyword pool for generating random keywords\n",
    "keyword_pool = [\n",
    "    \"graph processing\", \"property graph\", \"data quality\", \"data mining\",\n",
    "    \"machine learning\", \"distributed systems\", \"query optimization\",\n",
    "    \"graph databases\", \"big data\", \"semantic web\", \"information retrieval\",\n",
    "    \"knowledge graphs\", \"scalability\", \"neural networks\", \"clustering\",\n",
    "    \"text mining\", \"deep learning\", \"data integration\", \"cloud computing\",\n",
    "    \"edge computing\", \"stream processing\", \"natural language processing\",\n",
    "    \"transformer models\", \"recommender systems\", \"multi-modal learning\",\n",
    "    \"fairness in AI\", \"bias detection\", \"explainable AI\", \"federated learning\",\n",
    "    \"representation learning\", \"graph neural networks\", \"zero-shot learning\",\n",
    "    \"active learning\", \"anomaly detection\", \"semantic similarity\",\n",
    "    \"entity resolution\", \"ontology alignment\", \"blockchain applications\",\n",
    "    \"privacy-preserving ML\", \"data augmentation\", \"knowledge distillation\",\n",
    "    \"meta-learning\", \"reinforcement learning\", \"autonomous systems\",\n",
    "    \"computational social science\", \"medical informatics\", \"cybersecurity analytics\",\n",
    "    \"data provenance\", \"information diffusion\", \"social network analysis\"\n",
    "]\n",
    "\n",
    "def assign_keywords(pool, min_k=5, max_k=10):\n",
    "    return random.sample(pool, k=random.randint(min_k, max_k))\n",
    "\n",
    "# Add keywords column to each dataframe\n",
    "articles_df['keywords'] = articles_df.apply(lambda _: assign_keywords(keyword_pool), axis=1)\n",
    "inproceedings_df_fake['keywords'] = inproceedings_df_fake.apply(lambda _: assign_keywords(keyword_pool), axis=1)\n",
    "\n",
    "# Convert keyword lists to JSON strings\n",
    "articles_df['keywords'] = articles_df['keywords'].apply(json.dumps)\n",
    "inproceedings_df_fake['keywords'] = inproceedings_df_fake['keywords'].apply(json.dumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4340ab3",
   "metadata": {},
   "source": [
    "## Creating Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab02e24",
   "metadata": {},
   "source": [
    "First we will create a \"Reviewer Pool\". These are authors that have published more than 2 papers in conference/journals in our case. Then we will assign these reviewers to papers, taking the constrain into account. It is important to note that we filter for inproceedings equal to conference, as workshop's don't have reviewers according to the lab's statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb697fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated reviewer assignments for journal + conference papers only. Saved to review_edges.csv.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter only journal articles + conference-type inproceedings\n",
    "inproceedings_conferences = inproceedings_df_fake[inproceedings_df_fake['type'] == 'conference']\n",
    "all_papers_df = pd.concat([articles_df, inproceedings_conferences], ignore_index=True)\n",
    "\n",
    "# Step 2: Explode authors into individual rows\n",
    "author_paper_df = all_papers_df[['title', 'author']].copy()\n",
    "author_paper_df['author'] = author_paper_df['author'].str.split('|')\n",
    "author_paper_df = author_paper_df.explode('author').dropna()\n",
    "author_paper_df['author'] = author_paper_df['author'].str.strip()\n",
    "\n",
    "# Step 3: Build reviewer pool = authors with >= 2 papers\n",
    "author_counts = author_paper_df['author'].value_counts()\n",
    "reviewer_pool = author_counts[author_counts >= 2].index.tolist()\n",
    "\n",
    "# Step 4: Map each paper to its authors\n",
    "paper_authors = author_paper_df.groupby('title')['author'].apply(set).to_dict()\n",
    "\n",
    "# Step 5: Assign reviewers (3 different ones, not in author list)\n",
    "review_edges = []\n",
    "\n",
    "for title in all_papers_df['title']:\n",
    "    paper_auths = paper_authors.get(title, set())\n",
    "    eligible_reviewers = list(set(reviewer_pool) - paper_auths)\n",
    "\n",
    "    if len(eligible_reviewers) >= 3:\n",
    "        reviewers = random.sample(eligible_reviewers, 3)\n",
    "        for reviewer in reviewers:\n",
    "            review_edges.append({'reviewer': reviewer, 'paper': title})\n",
    "\n",
    "# Step 6: Create DataFrame and save to CSV\n",
    "review_edges_df = pd.DataFrame(review_edges)\n",
    "review_edges_df.to_csv('review_edges.csv', sep=';', index=False)\n",
    "\n",
    "print(\"‚úÖ Generated reviewer assignments for journal + conference papers only. Saved to review_edges.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b66fe8",
   "metadata": {},
   "source": [
    "# Exporting final csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f06e9",
   "metadata": {},
   "source": [
    "Checking for duplicate titles before exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82bf27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Duplicate titles in articles_df: 2 rows\n",
      "üîé Duplicate titles in inproceedings_df_fake: 4 rows\n",
      "Duplicated titles in articles_df:\n",
      "['CIGRE Austria Next Generation Network.']\n",
      "Duplicated titles in inproceedings_df_fake:\n",
      "['International conference on computational science, ICCS 2010 data-driven pill monitoring.'\n",
      " 'Workshop on tools for program development and analysis in computational science.']\n"
     ]
    }
   ],
   "source": [
    "# Check duplicates in articles_df\n",
    "duplicated_articles = articles_df[articles_df['title'].duplicated(keep=False)]\n",
    "\n",
    "# Check duplicates in inproceedings_df_fake\n",
    "duplicated_inproceedings = inproceedings_df_fake[inproceedings_df_fake['title'].duplicated(keep=False)]\n",
    "\n",
    "# Report\n",
    "print(f\"üîé Duplicate titles in articles_df: {duplicated_articles.shape[0]} rows\")\n",
    "print(f\"üîé Duplicate titles in inproceedings_df_fake: {duplicated_inproceedings.shape[0]} rows\")\n",
    "\n",
    "# If needed, print the titles\n",
    "if not duplicated_articles.empty:\n",
    "    print(\"Duplicated titles in articles_df:\")\n",
    "    print(duplicated_articles['title'].unique())\n",
    "\n",
    "if not duplicated_inproceedings.empty:\n",
    "    print(\"Duplicated titles in inproceedings_df_fake:\")\n",
    "    print(duplicated_inproceedings['title'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9ddd7",
   "metadata": {},
   "source": [
    "Checking these 3 special cases we have the same papers just for a different venue. We will keep the first occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5ee4079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exported clean CSVs with no duplicate titles.\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate titles, keeping the first occurrence\n",
    "articles_df = articles_df.drop_duplicates(subset='title', keep='first')\n",
    "inproceedings_df_fake = inproceedings_df_fake.drop_duplicates(subset='title', keep='first')\n",
    "\n",
    "# Then export safely\n",
    "articles_df.to_csv('dblp_article_clean.csv', sep=';', index=False)\n",
    "inproceedings_df_fake.to_csv('dblp_inproceedings_clean.csv', sep=';', index=False)\n",
    "\n",
    "print(\"‚úÖ Exported clean CSVs with no duplicate titles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "411f29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No duplicates in articles_df.\n",
      "‚úÖ No duplicates in inproceedings_df_fake.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in articles_df\n",
    "has_duplicates_articles = articles_df['title'].duplicated().any()\n",
    "\n",
    "# Check for duplicates in inproceedings_df_fake\n",
    "has_duplicates_inproceedings = inproceedings_df_fake['title'].duplicated().any()\n",
    "\n",
    "# Report results\n",
    "if has_duplicates_articles:\n",
    "    print(\"‚ùå Duplicates still exist in articles_df!\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates in articles_df.\")\n",
    "\n",
    "if has_duplicates_inproceedings:\n",
    "    print(\"‚ùå Duplicates still exist in inproceedings_df_fake!\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates in inproceedings_df_fake.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
